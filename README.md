# Discogs Lakehouse (Local)

Run-based, Reproducible Data Platform

This project implements a local-first Discogs lakehouse built around run-based versioning, immutable snapshots, and atomic data promotion.

The system is designed to behave like a real production-grade data platform, while remaining fully reproducible on a single machine.

It prioritizes correctness, auditability, and safety over convenience.

===================================================

## Overview

The platform follows three fundamental rules:
	‚Ä¢	Data is immutable
	‚Ä¢	Every pipeline execution is versioned
	‚Ä¢	Only validated data can be published

Instead of overwriting datasets, every run produces a complete snapshot that can be re-queried at any time.

Publishing is performed by switching a single symbolic pointer.

This mirrors how modern lakehouse systems operate in production.

=====================================================

## Project structure

The architecture is intentionally split into two independent repositories, each with a clearly defined responsibility.


### 1) Infrastructure layer

Trino + Hive Metastore + external table bootstrap

Repository:
üëâ https://github.com/PabloPablo666/trino-hive-setup

Responsibilities
	‚Ä¢	Trino compute engine (stateless)
	‚Ä¢	Hive Metastore backed by Postgres
	‚Ä¢	External table registration
	‚Ä¢	Stable SQL contract for consumers

This layer owns query execution only.

It does not own data.

It can be destroyed and recreated at any time without affecting stored datasets.


### 2) Pipeline & validation layer

Discogs ingestion, transformation, validation, orchestration

Repository:
üëâ https://github.com/PabloPablo666/discogs_tools_refactor

Responsibilities
	‚Ä¢	Download Discogs dumps
	‚Ä¢	Stream-parse large XML files
	‚Ä¢	Write typed Parquet datasets
	‚Ä¢	Build analytical warehouse tables
	‚Ä¢	Run validation checks
	‚Ä¢	Generate audit reports
	‚Ä¢	Promote validated data atomically

This repository owns the entire data lifecycle.

=============================================================

## Core design principles

‚úÖ Run-based architecture

Every pipeline execution produces an immutable snapshot:
```text
hive-data/
‚îî‚îÄ‚îÄ _runs/
    ‚îî‚îÄ‚îÄ <run_id>/
```

Each run contains:
	‚Ä¢	canonical typed datasets
	‚Ä¢	derived warehouse datasets
	‚Ä¢	validation reports
	‚Ä¢	execution metadata

Nothing is overwritten.
Every run remains queryable forever.


‚úÖ Active pointer (publish layer)

Consumers never read directly from _runs.

Instead, a single symbolic link defines the published dataset:

hive-data/active -> _runs/2026-01__20260117_192144

Publishing consists of switching this pointer atomically.

Benefits:
	‚Ä¢	zero-downtime publishing
	‚Ä¢	instant rollback
	‚Ä¢	stable table paths in Trino
	‚Ä¢	no partial states ever visible


‚úÖ Immutable data, mutable pointer

Data never changes after creation.

Only the pointer moves.

This is the same principle used by:
	‚Ä¢	data warehouses
	‚Ä¢	lakehouse systems
	‚Ä¢	versioned datasets in production environments

===================================================

## Data layout

Physical storage (run snapshot)

```text
hive-data/
‚îî‚îÄ‚îÄ _runs/
    ‚îî‚îÄ‚îÄ <run_id>/
        ‚îú‚îÄ‚îÄ artists_v1_typed/
        ‚îú‚îÄ‚îÄ artist_aliases_v1_typed/
        ‚îú‚îÄ‚îÄ artist_memberships_v1_typed/
        ‚îú‚îÄ‚îÄ masters_v1_typed/
        ‚îú‚îÄ‚îÄ releases_v6/
        ‚îú‚îÄ‚îÄ labels_v10/
        ‚îú‚îÄ‚îÄ release_artists_v1/
        ‚îú‚îÄ‚îÄ release_label_xref_v1/
        ‚îú‚îÄ‚îÄ label_release_counts_v1/
        ‚îú‚îÄ‚îÄ genre_style_xref/
        ‚îî‚îÄ‚îÄ _reports/
```


All datasets are stored as Parquet only.

No mutable formats.
No partial overwrites.

==================================================

## Logical access (Trino)

Trino external tables always point to:
file:/data/hive-data/active/...

As a result:
	‚Ä¢	SQL never changes
	‚Ä¢	dashboards never change
	‚Ä¢	notebooks never change

Only the active pointer changes after promotion.

This decouples compute from storage completely.

==================================================

## Pipeline lifecycle

The ingestion pipeline is orchestrated using Digdag and follows a strict execution model.


### 1) Preflight
	‚Ä¢	validate environment variables
	‚Ä¢	verify dump availability
	‚Ä¢	compute deterministic run_id

The run ID is generated once and propagated to all tasks.


### 2) Download (optional)
	‚Ä¢	downloads Discogs dumps by month
	‚Ä¢	idempotent
	‚Ä¢	skips existing files safely


### 3) Ingest
	‚Ä¢	streaming XML parsing
	‚Ä¢	no full-file loading
	‚Ä¢	constant memory usage

Typed canonical datasets are written:
	‚Ä¢	artists
	‚Ä¢	labels
	‚Ä¢	masters
	‚Ä¢	releases
	‚Ä¢	relationships

Each entity is processed independently.


### 4) Build warehouse

Derived analytical datasets are generated:
	‚Ä¢	artist name mappings
	‚Ä¢	release‚Äìartist relationships
	‚Ä¢	release‚Äìlabel relationships
	‚Ä¢	label release aggregations
	‚Ä¢	genre and style normalization tables

These tables are optimized for analytics, not raw storage.


### 5) Run-level parquet sanity checks

Before promotion, filesystem-level validations are executed:
	‚Ä¢	required datasets exist
	‚Ä¢	directories are not empty
	‚Ä¢	basic structural integrity

If any check fails, the run is aborted.

Nothing is published.


### 6) Promote

If all validations pass:
active -> _runs/<run_id>

The previous pointer is preserved automatically:
active__prev_<timestamp>

Rollback is a single filesystem operation.


### 7) Post-promotion Trino sanity report

After promotion, Trino-based validations are executed on the active dataset:
	‚Ä¢	row counts
	‚Ä¢	null ratios
	‚Ä¢	orphan foreign keys
	‚Ä¢	duplicate keys
	‚Ä¢	cross-table integrity

Results are exported as CSV:
_runs/<run_id>/_reports/trino_sanity_active_<timestamp>.csv

This creates a permanent audit trail.

=============================================================

## Why this design matters

‚úî Reproducibility

Any historical run can be queried exactly as it was produced.

‚úî Safe experimentation

New dumps can be ingested without touching production data.

‚úî Atomic publishing

Consumers see either old data or new data. Never partial states.

‚úî Rollback

One symlink switch.

‚úî Auditability

Every run produces structured, timestamped validation reports.

‚úî Infrastructure independence

Trino and Hive can be rebuilt freely without data loss.

==========================================================

## What this project is not
	‚Ä¢	not a toy ETL
	‚Ä¢	not overwrite-based ingestion
	‚Ä¢	not a one-off XML parser
	‚Ä¢	not ‚Äújust some Parquet files‚Äù

It behaves like a real lakehouse pipeline.

===========================================================

## Legal note

Discogs data is subject to Discogs licensing terms.

This project:
	‚Ä¢	does not distribute Discogs datasets
	‚Ä¢	does not ship dumps
	‚Ä¢	focuses exclusively on data engineering architecture and patterns
